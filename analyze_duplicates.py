"""
Script ƒë·ªÉ ph√¢n t√≠ch k·∫øt qu·∫£ duplicate detection
Gi√∫p hi·ªÉu t·∫°i sao t·ª∑ l·ªá unique qu√° th·∫•p
"""

import csv
import sys
from collections import defaultdict

def analyze_duplicates(duplicates_csv: str, unique_csv: str):
    """Ph√¢n t√≠ch file duplicates v√† unique"""
    
    print("="*70)
    print("üìä PH√ÇN T√çCH K·∫æT QU·∫¢ DUPLICATE DETECTION")
    print("="*70)
    
    # ƒê·ªçc duplicates
    duplicates = []
    similarity_scores = []
    original_counts = defaultdict(int)
    
    try:
        with open(duplicates_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                duplicates.append(row)
                similarity = float(row['similarity'])
                similarity_scores.append(similarity)
                original_counts[row['original_job_id']] += 1
    except FileNotFoundError:
        print(f"‚ùå File kh√¥ng t√¨m th·∫•y: {duplicates_csv}")
        return
    
    # ƒê·ªçc unique
    unique_count = 0
    try:
        with open(unique_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            unique_count = sum(1 for _ in reader)
    except FileNotFoundError:
        print(f"‚ùå File kh√¥ng t√¨m th·∫•y: {unique_csv}")
        return
    
    print(f"\nüìà TH·ªêNG K√ä:")
    print(f"   Unique videos: {unique_count}")
    print(f"   Duplicates found: {len(duplicates)}")
    
    if similarity_scores:
        print(f"\nüìä SIMILARITY SCORE DISTRIBUTION:")
        print(f"   Min: {min(similarity_scores):.6f}")
        print(f"   Max: {max(similarity_scores):.6f}")
        print(f"   Avg: {sum(similarity_scores)/len(similarity_scores):.6f}")
        print(f"   Median: {sorted(similarity_scores)[len(similarity_scores)//2]:.6f}")
        
        # Ph√¢n lo·∫°i theo threshold
        below_995 = sum(1 for s in similarity_scores if s < 0.995)
        above_995 = len(similarity_scores) - below_995
        
        print(f"\n   ‚ö†Ô∏è  V·ªõi threshold 0.995:")
        print(f"      - Duplicates >= 0.995: {above_995} ({above_995/len(similarity_scores)*100:.1f}%)")
        print(f"      - Duplicates < 0.995: {below_995} ({below_995/len(similarity_scores)*100:.1f}%)")
        print(f"      ‚Üí {below_995} duplicates s·∫Ω B·ªä B·ªé QUA v·ªõi threshold 0.995!")
        
        # Ph√¢n lo·∫°i theo threshold 0.98
        below_98 = sum(1 for s in similarity_scores if s < 0.98)
        above_98 = len(similarity_scores) - below_98
        
        print(f"\n   ‚úÖ V·ªõi threshold 0.98:")
        print(f"      - Duplicates >= 0.98: {above_98} ({above_98/len(similarity_scores)*100:.1f}%)")
        print(f"      - Duplicates < 0.98: {below_98} ({below_98/len(similarity_scores)*100:.1f}%)")
    
    # Top originals (videos c√≥ nhi·ªÅu duplicates nh·∫•t)
    if original_counts:
        print(f"\nüîù TOP 10 VIDEOS C√ì NHI·ªÄU DUPLICATES NH·∫§T:")
        sorted_originals = sorted(original_counts.items(), key=lambda x: x[1], reverse=True)
        for i, (job_id, count) in enumerate(sorted_originals[:10], 1):
            print(f"   {i}. {job_id}: {count} duplicates")
    
    # Ph√¢n t√≠ch URL patterns
    print(f"\nüîç PH√ÇN T√çCH URL PATTERNS:")
    url_patterns = defaultdict(int)
    for dup in duplicates:
        url = dup['duplicate_url']
        if 'videoplayback' in url:
            url_patterns['Google CDN'] += 1
        elif 'flashtalking.com' in url:
            url_patterns['Flashtalking'] += 1
        elif 'fptplay.net' in url:
            url_patterns['FPT Play'] += 1
        elif 'b-cdn.net' in url:
            url_patterns['Bunny CDN'] += 1
        else:
            url_patterns['Other'] += 1
    
    for pattern, count in sorted(url_patterns.items(), key=lambda x: x[1], reverse=True):
        print(f"   {pattern}: {count} duplicates ({count/len(duplicates)*100:.1f}%)")
    
    print("\n" + "="*70)
    print("üí° KHUY·∫æN NGH·ªä:")
    print("="*70)
    
    if similarity_scores:
        below_995_pct = below_995 / len(similarity_scores) * 100
        if below_995_pct > 20:
            print(f"   ‚ö†Ô∏è  {below_995_pct:.1f}% duplicates c√≥ similarity < 0.995")
            print(f"   ‚Üí N√™n gi·∫£m threshold xu·ªëng 0.98-0.99 ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c nhi·ªÅu duplicates h∆°n")
        
        if below_98 > 0:
            below_98_pct = below_98 / len(similarity_scores) * 100
            print(f"   ‚ö†Ô∏è  {below_98_pct:.1f}% duplicates c√≥ similarity < 0.98")
            print(f"   ‚Üí C√≥ th·ªÉ c√≥ false positives n·∫øu gi·∫£m threshold qu√° th·∫•p")
    
    print(f"\n   üìù Th·ª≠ ch·∫°y l·∫°i v·ªõi:")
    print(f"      python search_duplicates_aggregated.py --cosine_thresh 0.98 --chunk_start 0 --chunk_end 23000")
    print(f"      python search_duplicates_aggregated.py --cosine_thresh 0.99 --chunk_start 0 --chunk_end 23000")
    print("\n" + "="*70)


if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python analyze_duplicates.py <duplicates_csv> <unique_csv>")
        print("Example: python analyze_duplicates.py duplicates_chunk_0_23000.csv FINAL_RESULT_chunk_0_23000.csv")
        sys.exit(1)
    
    duplicates_csv = sys.argv[1]
    unique_csv = sys.argv[2]
    
    analyze_duplicates(duplicates_csv, unique_csv)


