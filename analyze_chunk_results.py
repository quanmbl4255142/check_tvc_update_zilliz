"""
Script ƒë·ªÉ ph√¢n t√≠ch k·∫øt qu·∫£ chunk v√† t√¨m ra t·∫°i sao c√≥ nhi·ªÅu videos "m·∫•t t√≠ch"
"""

import csv
import sys
from collections import defaultdict

def analyze_chunk_results(unique_csv, duplicates_csv, chunk_start, chunk_end):
    """Ph√¢n t√≠ch k·∫øt qu·∫£ chunk ƒë·ªÉ t√¨m videos b·ªã thi·∫øu"""
    
    print(f"\n{'='*80}")
    print(f"üìä PH√ÇN T√çCH K·∫æT QU·∫¢ CHUNK {chunk_start}-{chunk_end}")
    print(f"{'='*80}\n")
    
    # ƒê·ªçc FINAL_RESULT (unique videos)
    unique_job_ids = set()
    unique_urls = set()
    
    try:
        with open(unique_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get('decoded_url', '').strip().strip('"')
                if url:
                    unique_urls.add(url)
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc {unique_csv}: {e}")
        return
    
    print(f"üìÑ FINAL_RESULT.csv:")
    print(f"   - S·ªë URLs unique: {len(unique_urls)}")
    
    # ƒê·ªçc duplicates.csv
    duplicate_job_ids = set()
    cross_chunk_duplicates = 0
    within_chunk_duplicates = 0
    duplicate_urls = set()
    original_job_ids_in_chunk = set()
    original_job_ids_out_chunk = set()
    
    try:
        with open(duplicates_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                dup_job_id = row.get('duplicate_job_id', '').strip()
                orig_job_id = row.get('original_job_id', '').strip()
                dup_url = row.get('duplicate_url', '').strip().strip('"')
                orig_url = row.get('original_url', '').strip().strip('"')
                
                if dup_job_id:
                    duplicate_job_ids.add(dup_job_id)
                    duplicate_urls.add(dup_url)
                
                # Ki·ªÉm tra cross-chunk
                if '[CROSS-CHUNK:' in orig_url or orig_url.startswith('[CROSS-CHUNK:'):
                    cross_chunk_duplicates += 1
                    original_job_ids_out_chunk.add(orig_job_id)
                else:
                    within_chunk_duplicates += 1
                    # Extract job_id t·ª´ original_job_id
                    if orig_job_id.startswith('url_'):
                        try:
                            job_num = int(orig_job_id.split('_')[1])
                            if chunk_start <= job_num < chunk_end:
                                original_job_ids_in_chunk.add(orig_job_id)
                        except:
                            pass
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc {duplicates_csv}: {e}")
        return
    
    print(f"\nüìÑ duplicates.csv:")
    print(f"   - T·ªïng s·ªë duplicates: {len(duplicate_job_ids)}")
    print(f"   - Within-chunk duplicates: {within_chunk_duplicates}")
    print(f"   - Cross-chunk duplicates: {cross_chunk_duplicates}")
    print(f"   - Original job_ids trong chunk: {len(original_job_ids_in_chunk)}")
    print(f"   - Original job_ids ngo√†i chunk: {len(original_job_ids_out_chunk)}")
    
    # T√≠nh to√°n
    expected_total = chunk_end - chunk_start
    accounted_for = len(unique_urls) + len(duplicate_job_ids)
    missing = expected_total - accounted_for
    
    print(f"\nüìä T·ªîNG K·∫æT:")
    print(f"   - Expected videos (chunk {chunk_start}-{chunk_end}): {expected_total}")
    print(f"   - Unique videos (FINAL_RESULT): {len(unique_urls)}")
    print(f"   - Duplicate videos (duplicates.csv): {len(duplicate_job_ids)}")
    print(f"   - T·ªïng ƒë√£ t√≠nh: {accounted_for}")
    print(f"   - ‚ö†Ô∏è  THI·∫æU: {missing} videos ({missing/expected_total*100:.1f}%)")
    
    # Ph√¢n t√≠ch nguy√™n nh√¢n
    print(f"\nüîç PH√ÇN T√çCH NGUY√äN NH√ÇN:")
    print(f"\n   1. Cross-chunk duplicates:")
    print(f"      - {cross_chunk_duplicates} videos b·ªã ph√°t hi·ªán l√† duplicate c·ªßa video ngo√†i chunk")
    print(f"      - Nh·ªØng video n√†y KH√îNG c√≥ trong FINAL_RESULT (ƒë√∫ng)")
    print(f"      - Nh∆∞ng ch·ªâ c√≥ {cross_chunk_duplicates} videos trong duplicates.csv")
    print(f"      - ‚ö†Ô∏è  C√≥ th·ªÉ c√≥ nhi·ªÅu videos b·ªã duplicate nh∆∞ng kh√¥ng ƒë∆∞·ª£c ghi v√†o file!")
    
    print(f"\n   2. Possible issues:")
    print(f"      a) Script ch·ªâ t√¨m TOP_K (m·∫∑c ƒë·ªãnh 10) duplicates g·∫ßn nh·∫•t")
    print(f"         ‚Üí N·∫øu video c√≥ > 10 duplicates, ch·ªâ c√≥ 10 ƒë∆∞·ª£c ph√°t hi·ªán")
    print(f"      b) N·∫øu t·∫•t c·∫£ TOP_K duplicates ƒë·ªÅu ngo√†i chunk:")
    print(f"         ‚Üí Video s·∫Ω b·ªã ƒë√°nh d·∫•u cross-chunk duplicate")
    print(f"         ‚Üí Nh∆∞ng c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c ghi v√†o duplicates.csv ƒë·∫ßy ƒë·ªß")
    print(f"      c) Auto-clean c√≥ th·ªÉ lo·∫°i b·ªè nhi·ªÅu URLs:")
    print(f"         ‚Üí N·∫øu --auto_clean ƒë∆∞·ª£c b·∫≠t, nhi·ªÅu URLs c√≥ th·ªÉ b·ªã lo·∫°i")
    print(f"         ‚Üí Ki·ªÉm tra file invalid_urls_chunk_*.csv")
    
    print(f"\n   3. Recommendation:")
    print(f"      - Ki·ªÉm tra log khi ch·∫°y script ƒë·ªÉ xem:")
    print(f"        + S·ªë l∆∞·ª£ng videos loaded t·ª´ chunk")
    print(f"        + S·ªë l∆∞·ª£ng cross-chunk duplicates ƒë∆∞·ª£c ph√°t hi·ªán")
    print(f"        + S·ªë l∆∞·ª£ng standalone videos")
    print(f"        + S·ªë l∆∞·ª£ng invalid URLs (n·∫øu auto-clean)")
    print(f"      - Ch·∫°y l·∫°i v·ªõi --top_k l·ªõn h∆°n (v√≠ d·ª•: 50) ƒë·ªÉ t√¨m nhi·ªÅu duplicates h∆°n")
    print(f"      - Ki·ªÉm tra file invalid_urls n·∫øu c√≥")
    
    # Ki·ªÉm tra invalid URLs file
    invalid_csv = f"invalid_urls_chunk_{chunk_start}_{chunk_end}.csv"
    try:
        with open(invalid_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            invalid_count = sum(1 for _ in reader)
            if invalid_count > 0:
                print(f"\n   4. Invalid URLs (auto-clean):")
                print(f"      - T√¨m th·∫•y {invalid_count} invalid URLs trong {invalid_csv}")
                print(f"      - Nh·ªØng URLs n√†y ƒë√£ b·ªã lo·∫°i kh·ªèi FINAL_RESULT")
                print(f"      - ƒê√¢y c√≥ th·ªÉ l√† m·ªôt ph·∫ßn c·ªßa {missing} videos b·ªã thi·∫øu")
    except FileNotFoundError:
        print(f"\n   4. Invalid URLs:")
        print(f"      - Kh√¥ng t√¨m th·∫•y file {invalid_csv}")
        print(f"      - C√≥ th·ªÉ --auto_clean kh√¥ng ƒë∆∞·ª£c b·∫≠t")
    except Exception as e:
        print(f"\n   4. Invalid URLs:")
        print(f"      - L·ªói ƒë·ªçc file: {e}")
    
    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    if len(sys.argv) < 5:
        print("Usage: python analyze_chunk_results.py <unique_csv> <duplicates_csv> <chunk_start> <chunk_end>")
        print("Example: python analyze_chunk_results.py FINAL_RESULT_chunk_0_5000.csv duplicates_chunk_0_5000.csv 0 5000")
        sys.exit(1)
    
    unique_csv = sys.argv[1]
    duplicates_csv = sys.argv[2]
    chunk_start = int(sys.argv[3])
    chunk_end = int(sys.argv[4])
    
    analyze_chunk_results(unique_csv, duplicates_csv, chunk_start, chunk_end)

